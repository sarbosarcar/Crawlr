# -*- coding: utf-8 -*-
"""Evaluation of Responses.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13yWtlQmS5_4jxcV6lL6dO2mNN1ZC_ems
"""

#@title Installing dependencies
# !pip install textstat

"""# Functions to compute metrics"""

#@title Importing necessary libraries
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel
import numpy as np
import math
from sentence_transformers import SentenceTransformer, util
import datetime

import textstat

#@title Contextuality
def contextuality(query, document, model_name="all-MiniLM-L6-v2", top_k=5):
    """
    Computes the contextuality score between a query and a large document
    by dynamically chunking the document into larger, meaningful segments.

    Args:
        query (str): The input query.
        document (str): The large context document.
        model_name (str): SentenceTransformer model name.
        top_k (int): Number of top-relevant chunks to consider.

    Returns:
        float: Contextuality score (0 to 1).
    """
    # Load the model
    model = SentenceTransformer(model_name)

    # Encode the query
    query_embedding = model.encode(query, normalize_embeddings=True)

    # Dynamic chunking: Use chunks ~2x the query length
    chunk_size = min(len(query) * 2, 512)  # Cap chunk size at 512 tokens to avoid excessive memory usage
    sentences = document.split()  # Tokenize document into words
    chunks = [" ".join(sentences[i : i + chunk_size]) for i in range(0, len(sentences), chunk_size)]

    # Compute embeddings for all chunks
    chunk_embeddings = model.encode(chunks, normalize_embeddings=True)

    # Compute similarity scores between query and each chunk
    similarities = util.cos_sim(query_embedding, chunk_embeddings)[0].cpu().numpy()

    # Get top-k most relevant chunks
    top_k_indices = np.argsort(similarities)[-top_k:]
    top_k_embeddings = chunk_embeddings[top_k_indices]

    # Compute weighted mean of top-k chunk embeddings
    weights = similarities[top_k_indices]  # Use similarity scores as weights
    weighted_mean_embedding = np.average(top_k_embeddings, axis=0, weights=weights)

    # Compute final similarity score between query and weighted mean of top chunks
    similarity_score = util.cos_sim(query_embedding, weighted_mean_embedding).item()

    return similarity_score

#@title Coherence (C)
def coherence(query, response, model_name="all-MiniLM-L6-v2"):
    """
    Calculates the relevance between a query and a response using cosine similarity.

    Args:
        query: The search query (string).
        response: The LLM response (string).
        model_name: The name of the SentenceTransformer model to use.

    Returns:
        The cosine similarity score between the query and response embeddings.
    """
    # Load SentenceTransformer model
    model = SentenceTransformer(model_name)

    # Encode query and response
    query_embedding = model.encode(query, convert_to_tensor=True)
    response_embedding = model.encode(response, convert_to_tensor=True)

    # Compute cosine similarity
    similarity_score = util.cos_sim(query_embedding, response_embedding).item()

    return similarity_score

#@title Recency (r)
def date_difference(input_date_str):
  """Calculates the difference between an input date and the present date.

  Args:
    input_date_str: A string representing the input date in YYYY-MM-DD format.

  Returns:
    A timedelta object representing the difference, or None if the input date is invalid.
    Also prints the difference in days.
  """
  try:
    input_date = datetime.datetime.strptime(input_date_str, "%Y-%m-%d").date()
    present_date = datetime.date.today()
    difference = present_date - input_date
    # print(f"Difference: {difference.days} days")
    return difference
  except ValueError:
    print("Invalid date format. Please use YYYY-MM-DD.")
    return None

def difference(input_date, lambda_=0.15):
  diff = (date_difference(input_date).total_seconds())/(60*60*24)
  return np.exp(-lambda_*(diff))

def recency(dates, lambda_=0.15):
  return np.mean([difference(date, lambda_) for date in dates])

def recency_diff(days, lambda_=0.15):
  return np.mean([np.exp(-lambda_*diff) for diff in days])

#@title Scaled Entropy
def scaled_entropy(response_text, model_name="sentence-transformers/all-MiniLM-L6-v2"):
    # Load model and tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)

    # Tokenize input
    inputs = tokenizer(response_text, return_tensors="pt", padding=True, truncation=True)

    with torch.no_grad():
        outputs = model(**inputs)

    embeddings = outputs.last_hidden_state  # Shape: (batch_size, seq_len, hidden_dim)

    # Compute probability approximation using softmax over norms
    norms = torch.norm(embeddings, dim=-1)  # Compute L2 norm per token
    probs = torch.nn.functional.softmax(norms, dim=-1)  # Convert to probability distribution

    # Compute Shannon entropy
    epsilon = 1e-10  # Small constant to avoid log(0)
    entropy = -torch.sum(probs * torch.log2(probs + epsilon), dim=-1).mean().item()

    # Normalize entropy
    vocab_size = embeddings.shape[-1]  # Approximate vocab size from embedding dim
    max_entropy = np.log2(vocab_size)  # Theoretical max entropy
    scaled_E = entropy / max_entropy  # Scale to match typical range
    # print(max_entropy, entropy)
    return scaled_E

#@title Groundedness (G)
def groundedness(answer, sources):
  return contextuality(answer, sources)

#@title Readability (R)
def fres(text):
    return textstat.flesch_reading_ease(text)

def readability(text):
  return (1 - abs(0.01 * fres(text) - 0.65))

#@title Alpha
def alpha(A, T, r):
  return A*T*r

#@title Beta
def beta(C, G, E):
  return C*G*(1-E)

#@title Gamma
def gamma(L, R, lambda_=0.03):
  return (1-np.exp(-L*lambda_))*R

#@title Omega
def omega(a, b, g, s=0.5):
  return (s*a+(1-s)*b)**g

import re

def parse_multiple_text_blocks(text):
    """
    Parses multiple structured 'Title:', 'Link:', and 'Content:' blocks from a string.

    Args:
        text (str): Input string containing multiple Title, Link, and Content sections.

    Returns:
        list: A list of dictionaries, each containing 'title', 'link', and 'content'.
    """
    pattern = r"Title:(.*?)\nLink:(.*?)\nContent:(.*?)(?=\nTitle:|\Z)"  # Matches multiple blocks
    matches = re.findall(pattern, text, re.DOTALL)

    parsed_entries = []
    s = set()
    for match in matches:
      if match[1].strip() in s:
        continue
      s.add(match[1].strip())
      parsed_entries.append({
          "title": match[0].strip(),
          "link": f"https://www.google.com/search?tbs=cdr%3A1%2Ccd_min%3A1%2F1%2F2000&q=site%3Ahttp%3A%2F%2F{match[1].strip()}&safe=active&gws_rd=ssl",
          "content": match[2].strip()  # Preserves newlines
      })

    return parsed_entries
